{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "\n",
    "#Academicç”³è«‹å¾Œã«ç”Ÿæˆã•ã‚Œã‚‹\"Bearer Token(BT)\"\n",
    "BT = '###############'\n",
    "\n",
    "\n",
    "TWEET_LIMIT_you_set = 1000 #### Defult = 1000 for test\n",
    "\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "#ä¸ãˆã‚‰ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆå–å¾—æ¡ä»¶(query_params)ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°\n",
    "def make_parm(keyword, start_time, end_time):\n",
    "\n",
    "    #æ¤œç´¢æ¡ä»¶(ä»Šå›ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€é–‹å§‹æ™‚åˆ»ã€ä¿®äº†æ™‚åˆ»ã®ã¿)\n",
    "    query_params = {'query': keyword ,\n",
    "                    'tweet.fields': 'created_at,referenced_tweets',\n",
    "                    'expansions': 'author_id,geo.place_id,in_reply_to_user_id',\n",
    "                    'start_time': start_time,\n",
    "                    'end_time': end_time,\n",
    "                    'user.fields': 'created_at,description,location,username',\n",
    "                    'max_results':500,  #ä¸€å›ã®queryã¯ï¼•ï¼ï¼ã§ä¸Šé™ã‚‰ã—ã„\n",
    "                    'next_token' : {} #æ¬¡ã®ãƒšãƒ¼ã‚¸ã«ã„ããŸã‚ã®param\n",
    "                   }\n",
    "    return query_params\n",
    "\n",
    "\n",
    "def create_headers(BT):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(BT)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params):\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "    #print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        #raise Exception(response.status_code, response.text)\n",
    "    \n",
    "        print(response.status_code, response.text)\n",
    "        \n",
    "    return response   ######## to make use of  header information, return response instead of response.json()\n",
    "\n",
    "def convert_timezone(normalized_data):\n",
    "    normalized_data = normalized_data.copy().dropna(subset=['created_at'])\n",
    "    normalized_data.created_at = pd.to_datetime(normalized_data.created_at, utc=True)\n",
    "    normalized_data.index = pd.DatetimeIndex(normalized_data.created_at, name='created_at')\n",
    "    normalized_data.index = normalized_data.index.tz_convert('Asia/Tokyo')\n",
    "    \n",
    "    # sort dataframe by created time\n",
    "    \n",
    "    normalized_data.created_at = normalized_data.index\n",
    "    normalized_data = normalized_data.sort_index()\n",
    "    normalized_data = normalized_data.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # rename index\n",
    "    \n",
    "    print('Timezone convertion finished')\n",
    "    return normalized_data\n",
    "\n",
    "def store_checkpoint(normalized_data, keyword):\n",
    "    \n",
    "    # store collected data to csv in case of error occurrence\n",
    "    \n",
    "    savedir = './' # set your savedir here\n",
    "    \n",
    "    kwindex = keyword\n",
    "\n",
    "        \n",
    "    filename = kwindex + normalized_data.iloc[0]['created_at'].strftime('%Y%m%d%H%M') + '-' + \\\n",
    "               normalized_data.iloc[-1]['created_at'].strftime('%Y%m%d%H%M') + \\\n",
    "                '.csv'\n",
    "\n",
    "\n",
    "    savepath = savedir + filename    \n",
    "    \n",
    "    normalized_data.to_csv(path_or_buf = savepath)\n",
    "    print('Successfully saved at ' + savepath + ' at ' + \\\n",
    "              datetime.today().strftime(\"%Y/%m/%d-%H:%M\"))\n",
    "    \n",
    "def json_to_df(json_response):\n",
    "    \n",
    "    #print(json_response)\n",
    "    normalized_data = pd.json_normalize(json_response['data'])\n",
    "    normalized_users = pd.json_normalize(json_response['includes']['users']).set_index('id')\n",
    "     \n",
    "    normalized_users.rename({'id': 'author_id'}, inplace = True, axis = 1)\n",
    "    normalized_data.rename({'geo.place_id': 'place_id'}, inplace = True, axis = 1)\n",
    "            \n",
    "    normalized = normalized_data.join(normalized_users, on = 'author_id', how = 'outer', rsuffix = '_user')\n",
    "    normalized['text'] = normalized['text'].str.replace('\\n', ' ')     #textå†…ã®\\nã‚’å‰Šé™¤\n",
    "    normalized['text'] = normalized['text'].str.replace('@', ' @')     #@å‰ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ \n",
    "    \n",
    "    normalized['referenced_tweets_type'] = normalized['referenced_tweets'].apply(lambda x: x[0]['type'] if (np.all(pd.notnull(x))) else np.nan)\n",
    "    normalized['referenced_tweets_id'] = normalized['referenced_tweets'].apply(lambda x: x[0]['id'] if (np.all(pd.notnull(x))) else np.nan)\n",
    "    del normalized['referenced_tweets']\n",
    "\n",
    "            \n",
    "    return normalized\n",
    "\n",
    "\n",
    "#å®Ÿéš›ã«ãƒ„ã‚¤ãƒ¼ãƒˆå–å¾—ã‚’ã™ã‚‹é–¢æ•°(ä¸Šã®5ã¤ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã—ãªãŒã‚‰)\n",
    "def main(keyword, start_time, end_time):\n",
    "\n",
    "    count = 0\n",
    "    flag = True\n",
    "    TWEET_LIMIT = TWEET_LIMIT_you_set\n",
    "    \n",
    "    \n",
    "    normalized_data_old = pd.DataFrame(columns=['created_at','id', 'text', 'author_id', \n",
    "                                            'referenced_tweets_type',  'referenced_tweets_id',\n",
    "                                            'in_reply_to_user_id' ,\n",
    "                                            'place_id', 'description',\n",
    "                                            'name', 'verified', 'username', 'created_at_user',\n",
    "                                            'location'])\n",
    "    \n",
    "    #cols = ['created_at', 'id', ' text']\n",
    "    #normalized_data_old = pd.DataFrame(index=[], columns=cols)#create checkpoint dataframe\n",
    "    \n",
    "    query_params = make_parm(keyword, start_time, end_time)#ã“ã“ã§å–å¾—æ¡ä»¶ãŒè¿”ã•ã‚Œã‚‹\n",
    "    \n",
    "    while flag:\n",
    "        if count >= TWEET_LIMIT:\n",
    "            break\n",
    "    \n",
    "        headers = create_headers(BT)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        response = connect_to_endpoint(search_url, headers, query_params)\n",
    "        \n",
    "        json_response = response.json()  \n",
    "        \n",
    "        \n",
    "        #print(json_response['data']) \n",
    "        #print(json_response) \n",
    "        \n",
    "        normalized_data = json_to_df(json_response)\n",
    "        \n",
    "        normalized_data_new = pd.concat([normalized_data_old, normalized_data], ignore_index=True)\n",
    "         \n",
    "        normalized_data_old = normalized_data_new\n",
    "        \n",
    "        print(\"total:\" + str(len(normalized_data_old)) + \"tweets\")\n",
    "    \n",
    "\n",
    "        result_count = json_response['meta']['result_count']\n",
    "        \n",
    "        if 'next_token' in json_response['meta']:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            query_params['next_token'] = next_token\n",
    "            count += result_count\n",
    "            \n",
    "            time.sleep(3)\n",
    "            \n",
    "            response = connect_to_endpoint(search_url, headers, query_params)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            elif response.status_code == 429:\n",
    "                \n",
    "                # store checkpoints when error occurs\n",
    "                \n",
    "                normalized_data_last = convert_timezone(normalized_data_old)\n",
    "                store_checkpoint(normalized_data_last, keyword)\n",
    "                \n",
    "                # from https://github.com/mammalofski/Twitter-Scraper/blob/main/Scraper.py\n",
    "                \n",
    "                print('too many requests ... ')\n",
    "                print('the header is ', response.headers)\n",
    "                throttle_end_timestamp = int(response.headers.get('x-rate-limit-reset'))\n",
    "                throttle_end_time = datetime.strftime(datetime.fromtimestamp(throttle_end_timestamp), \"%H:%M:%S\")\n",
    "                time_to_wait = int(throttle_end_timestamp - datetime.now().timestamp()) + 5\n",
    "                print('lets rest for', time_to_wait, 'seconds and wake up at', throttle_end_time)\n",
    "                print('sleeping ...')\n",
    "                time.sleep(time_to_wait)\n",
    "                response = connect_to_endpoint(search_url, headers, query_params)\n",
    "                \n",
    "            else:\n",
    "                normalized_data_last = convert_timezone(normalized_data_old)\n",
    "                store_checkpoint(normalized_data_last, keyword)\n",
    "                \n",
    "                print('un expected error : ')\n",
    "                print(response.status_code, response.text)\n",
    "                print('sleep for 15 min')\n",
    "                time.sleep( 15 * 60 + 10)  # rest 15min \n",
    "                print('restart at ' + datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M:%S\"))\n",
    "                \n",
    "                response = connect_to_endpoint(search_url, headers, query_params)\n",
    "             \n",
    "            time.sleep(3)  # rate limit = 1 request/1 sec\n",
    "            \n",
    "            json_response = response.json()\n",
    "            \n",
    "            \n",
    "            #append the current page of results to lists\n",
    "            \n",
    "            \n",
    "            normalized_data = json_to_df(json_response)\n",
    "        \n",
    "            normalized_data_new = pd.concat([normalized_data_old, normalized_data], ignore_index=True)\n",
    "            \n",
    "            normalized_data_old = normalized_data_new\n",
    "            \n",
    "            print(\"total:\" + str(len(normalized_data_old)) + \"tweets\")\n",
    "\n",
    "        else:\n",
    "            flag = False\n",
    "            print('Last page of query results')\n",
    "            \n",
    "            \n",
    "    \n",
    "    # convert UTC +0 timezone to UTC +9 timezone\n",
    "    normalized_data_last = convert_timezone(normalized_data_old)\n",
    "    \n",
    "    if count % 500 <= 100:\n",
    "        print('Finished collecting tweets at ' + str(normalized_data_last.iloc[-1]['created_at']))\n",
    "    \n",
    "    \n",
    "    # store checkpoint to csv\n",
    "    \n",
    "    store_checkpoint(normalized_data_last, keyword)\n",
    "    \n",
    "    #normalized_data_last.to_csv('../FullArchiveData_2021Apr/T20200331-0501.csv')#csvãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:12tweets\n",
      "Last page of query results\n",
      "Timezone convertion finished\n",
      "Finished collecting tweets at 2022-03-16 03:51:57+09:00\n",
      "Successfully saved at ./ç ”ç©¶æ¥½ã—ã„202203151321-202203160351.csv at 2022/03/22-20:26\n"
     ]
    }
   ],
   "source": [
    "main(keyword=\"ç ”ç©¶æ¥½ã—ã„\", start_time=\"2022-03-15T00:00:00Z\", end_time=\"2022-03-16T00:00:00Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./ç ”ç©¶æ¥½ã—ã„202203151321-202203160351.csv',usecols=['created_at','id', 'text', 'author_id'],dtype={'id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-15 13:21:26+09:00</td>\n",
       "      <td>1503587147898830852</td>\n",
       "      <td>è‡ªåˆ†ã®é¡”ã«é£½ãã¦ããŸã‘ã©ã€å¤§ããæ•´å½¢ã—ãŸã„æ‰€ã¯ç„¡ã„ã‹ã‚‰ã€ã‚¦ã‚£ãƒƒã‚°ã‚„æ™®æ®µç€ãªã„æœè²·ã£ã¦ã¿ãŸã€‚ ...</td>\n",
       "      <td>1279852314195443712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-15 16:33:14+09:00</td>\n",
       "      <td>1503635417068826628</td>\n",
       "      <td>ã¿ã‚“ãªã‹ã‚‰ã®åŒ¿åè³ªå•ã‚’å‹Ÿé›†ä¸­ï¼  ã“ã‚“ãªè³ªå•ã«ç­”ãˆã¦ã‚‹ã‚ˆ â— sã§ã™ â— ãšã°ã‚Šã‚¤ãƒ¡ãƒ¼ã‚¸ã®ã„...</td>\n",
       "      <td>845446421109125121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-15 17:25:23+09:00</td>\n",
       "      <td>1503648543449714689</td>\n",
       "      <td>@maru_i5 å‹é”ã«ã‚·ãƒ£ãƒ‰ã‚¦ã§å¼•ã„ã¦ã¿ãŸã‚‰ï¼Ÿã£ã¦ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã•ã‚Œã¦ã‚„ã£ã¦ã¿ãŸã®ğŸ¥°ğŸ¥° ãƒ¡ã‚¤...</td>\n",
       "      <td>845613959432421376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-15 20:33:19+09:00</td>\n",
       "      <td>1503695836622254083</td>\n",
       "      <td>@Fkga_34 ãƒ¡ã‚¤ã‚¯ç ”ç©¶æ¥½ã—ã„å¤‰ã‚ã‚Œã‚‹ã‹ã‚‰æ¥½ã—ã„Ë™ğƒ·Ë™</td>\n",
       "      <td>1383084669881618433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-15 22:11:52+09:00</td>\n",
       "      <td>1503720639818973184</td>\n",
       "      <td>ã‚½ãƒ¼ãƒ´ã‚¡ç ”ç©¶æ¥½ã—ã„</td>\n",
       "      <td>2441593783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-03-15 22:13:23+09:00</td>\n",
       "      <td>1503721020758573058</td>\n",
       "      <td>ç ”ç©¶â†’æ¥½ã—ã„ã€‚</td>\n",
       "      <td>2479324278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-03-15 22:34:50+09:00</td>\n",
       "      <td>1503726416088924162</td>\n",
       "      <td>@mabo05201 å±±ã”é£¯ç ”ç©¶æ¥½ã—ã„ã§ã™ã‚ˆã­(à¹‘ËƒÌµá´—Ë‚Ìµ)</td>\n",
       "      <td>1428702171646685185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-03-15 23:31:06+09:00</td>\n",
       "      <td>1503740575795003394</td>\n",
       "      <td>ã‚½ãƒ¼ãƒ´ã‚¡ã®ãƒ€ãƒ¼ãƒ„ç ”ç©¶æ¥½ã—ã„ã‘ã©å®Ÿè·µã§ä½¿ãˆã‚‹ã‹ã¯ä¸å®‰ã€‚</td>\n",
       "      <td>852784310738341889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-03-16 03:51:57+09:00</td>\n",
       "      <td>1503806224206217216</td>\n",
       "      <td>ãƒãƒƒãƒ—ç ”ç©¶æ¥½ã—ã„ã‘ã©ã€è»Šæ¹§ãã¨ã‹ãƒ«ãƒ¼ãƒˆå–ã‚Šã¨ã‹ã¾ã§è€ƒãˆã‚‹ã¨ãƒã‚¸ã§è„³ç–²ã‚Œã‚‹ã€‚  æ”¹ã‚ã¦ãƒ™ãƒã‚­ã•...</td>\n",
       "      <td>1333503066373427200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at                   id  \\\n",
       "0  2022-03-15 13:21:26+09:00  1503587147898830852   \n",
       "1  2022-03-15 16:33:14+09:00  1503635417068826628   \n",
       "2  2022-03-15 17:25:23+09:00  1503648543449714689   \n",
       "3  2022-03-15 20:33:19+09:00  1503695836622254083   \n",
       "4  2022-03-15 22:11:52+09:00  1503720639818973184   \n",
       "5  2022-03-15 22:13:23+09:00  1503721020758573058   \n",
       "6  2022-03-15 22:34:50+09:00  1503726416088924162   \n",
       "7  2022-03-15 23:31:06+09:00  1503740575795003394   \n",
       "8  2022-03-16 03:51:57+09:00  1503806224206217216   \n",
       "\n",
       "                                                text            author_id  \n",
       "0  è‡ªåˆ†ã®é¡”ã«é£½ãã¦ããŸã‘ã©ã€å¤§ããæ•´å½¢ã—ãŸã„æ‰€ã¯ç„¡ã„ã‹ã‚‰ã€ã‚¦ã‚£ãƒƒã‚°ã‚„æ™®æ®µç€ãªã„æœè²·ã£ã¦ã¿ãŸã€‚ ...  1279852314195443712  \n",
       "1  ã¿ã‚“ãªã‹ã‚‰ã®åŒ¿åè³ªå•ã‚’å‹Ÿé›†ä¸­ï¼  ã“ã‚“ãªè³ªå•ã«ç­”ãˆã¦ã‚‹ã‚ˆ â— sã§ã™ â— ãšã°ã‚Šã‚¤ãƒ¡ãƒ¼ã‚¸ã®ã„...   845446421109125121  \n",
       "2   @maru_i5 å‹é”ã«ã‚·ãƒ£ãƒ‰ã‚¦ã§å¼•ã„ã¦ã¿ãŸã‚‰ï¼Ÿã£ã¦ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã•ã‚Œã¦ã‚„ã£ã¦ã¿ãŸã®ğŸ¥°ğŸ¥° ãƒ¡ã‚¤...   845613959432421376  \n",
       "3                      @Fkga_34 ãƒ¡ã‚¤ã‚¯ç ”ç©¶æ¥½ã—ã„å¤‰ã‚ã‚Œã‚‹ã‹ã‚‰æ¥½ã—ã„Ë™ğƒ·Ë™  1383084669881618433  \n",
       "4                                          ã‚½ãƒ¼ãƒ´ã‚¡ç ”ç©¶æ¥½ã—ã„           2441593783  \n",
       "5                                            ç ”ç©¶â†’æ¥½ã—ã„ã€‚           2479324278  \n",
       "6                    @mabo05201 å±±ã”é£¯ç ”ç©¶æ¥½ã—ã„ã§ã™ã‚ˆã­(à¹‘ËƒÌµá´—Ë‚Ìµ)  1428702171646685185  \n",
       "7                         ã‚½ãƒ¼ãƒ´ã‚¡ã®ãƒ€ãƒ¼ãƒ„ç ”ç©¶æ¥½ã—ã„ã‘ã©å®Ÿè·µã§ä½¿ãˆã‚‹ã‹ã¯ä¸å®‰ã€‚   852784310738341889  \n",
       "8  ãƒãƒƒãƒ—ç ”ç©¶æ¥½ã—ã„ã‘ã©ã€è»Šæ¹§ãã¨ã‹ãƒ«ãƒ¼ãƒˆå–ã‚Šã¨ã‹ã¾ã§è€ƒãˆã‚‹ã¨ãƒã‚¸ã§è„³ç–²ã‚Œã‚‹ã€‚  æ”¹ã‚ã¦ãƒ™ãƒã‚­ã•...  1333503066373427200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
